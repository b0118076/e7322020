{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6-ifF-lNDtc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYAkeT0NNDwf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYjUoxZaNozA"
   },
   "outputs": [],
   "source": [
    "#Pyspark\n",
    "\n",
    "pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = spark.read.csv(\"C:/Users/RITHVI/Desktop/pred_final/Log_Reg_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Perform descriptive statistics\n",
    "data.describe().show()\n",
    "\n",
    "# Step 4: Assign probability values (0 and 1) for class 0 and class 1 in 'Status' column\n",
    "data = data.withColumn(\"Status\", data[\"Status\"].cast(\"double\"))\n",
    "\n",
    "# Step 5: Convert categorical columns to numeric using StringIndexer\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\")\n",
    "    for col in [\"Country\", \"Platform\"]\n",
    "]\n",
    "indexers.append(StringIndexer(inputCol=\"Repeat_Visitor\", outputCol=\"Repeat_Visitor_index\", handleInvalid=\"keep\"))\n",
    "\n",
    "# Fit and transform the data with the StringIndexer\n",
    "for indexer in indexers:\n",
    "    data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Step 6: Prepare data for logistic regression\n",
    "# Select features and label columns\n",
    "feature_cols = [\"Country_index\", \"Age\", \"Repeat_Visitor_index\", \"Platform_index\", \"Web_pages_viewed\"]\n",
    "label_col = \"Status\"\n",
    "\n",
    "# Create a vector assembler to combine features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Step 7: Split the data into training and test datasets\n",
    "(training_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 8: Build and fit the logistic regression model\n",
    "lr = LogisticRegression(labelCol=label_col)\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# Step 9: Make predictions on the test dataset\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = predictions.groupBy(\"Status\", \"prediction\").count()\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix.show()\n",
    "\n",
    "# Precision, recall, and F1 score\n",
    "TP = predictions[(predictions.Status == 1) & (predictions.prediction == 1)].count()\n",
    "TN = predictions[(predictions.Status == 0) & (predictions.prediction == 0)].count()\n",
    "FP = predictions[(predictions.Status == 0) & (predictions.prediction == 1)].count()\n",
    "FN = predictions[(predictions.Status == 1) & (predictions.prediction == 0)].count()\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clqiJINgj6Xc"
   },
   "outputs": [],
   "source": [
    "#Superstore\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('/content/Superstore.xls')\n",
    "\n",
    "office_supplies = df[df['Category'] == 'Office Supplies']\n",
    "office_supplies['Order Date'] = pd.to_datetime(office_supplies['Order Date'])\n",
    "# Monthly sales\n",
    "monthly_sales = office_supplies.groupby(pd.Grouper(key='Order Date', freq='M')).sum()['Sales']\n",
    "\n",
    "# a. Stationarity check\n",
    "plt.plot(monthly_sales, label='Original')\n",
    "plt.plot(monthly_sales.rolling(window=12).mean(), label='Rolling Mean')\n",
    "plt.plot(monthly_sales.rolling(window=12).std(), label='Rolling Std')\n",
    "plt.legend()\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show()\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "result = adfuller(monthly_sales)\n",
    "print('ADF Statistic (a):', result[0])\n",
    "print('p-value (a):', result[1])\n",
    "print('Critical Values (a):', result[4])\n",
    "\n",
    "# b. Determine order of differencing, d\n",
    "d = 1\n",
    "\n",
    "plt.plot(monthly_sales)\n",
    "plt.title('Monthly Sales')\n",
    "plt.xlabel('Order Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# c. Determine order p for AR(p)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_pacf(monthly_sales, lags=min(24, len(monthly_sales)-1), alpha=0.05)\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n",
    "\n",
    "# Assuming p = 2\n",
    "p = 2\n",
    "\n",
    "# d. Determine order q for MA(q)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(monthly_sales, lags=30, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Assuming q = 2\n",
    "q = 2\n",
    "\n",
    "# e. Fit ARIMA model and forecast\n",
    "model = ARIMA(monthly_sales, order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=len(monthly_sales))\n",
    "\n",
    "# Plot original data and forecast\n",
    "plt.plot(monthly_sales, label='Original')\n",
    "plt.plot(monthly_sales.index, forecast, label='Forecast', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model accuracy\n",
    "mse = mean_squared_error(monthly_sales, forecast)\n",
    "print('Mean Squared Error (MSE) (e):', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsKQYjOT9KRd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37uhqToJkLgU"
   },
   "outputs": [],
   "source": [
    "#Spam\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/RITHVI/Desktop/pred_final/Spam.csv\", encoding='latin-1')\n",
    "\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/RITHVI/Desktop/pred_final/Spam.csv\", encoding='latin-1')\n",
    "\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(df['message'])\n",
    "\n",
    "\n",
    "y = df['label']\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x, y)\n",
    "\n",
    "def evaluate_classifier(clf, x_test, y_test):\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(clf, x, y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"precision: {precision:.2f}\")\n",
    "print(f\"recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHbKvNqn5mBi"
   },
   "outputs": [],
   "source": [
    "#India exchange rate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "data = pd.read_excel(\"C:/Users/RITHVI/Downloads/India_Exchange_Rate_Dataset.xls\")\n",
    "\n",
    "data['EXINUS'].plot(figsize=(12, 6), title='India Exchange Rate')\n",
    "plt.show()\n",
    "\n",
    "data['EXINUS'].rolling(window=12).mean().plot(figsize=(12, 6), title='India Exchange Rate with 12-month SMA')\n",
    "plt.show()\n",
    "\n",
    "data['EXINUS'].ewm(span=12).mean().plot(figsize=(12, 6), title='India Exchange Rate with 12-month EWMA')\n",
    "plt.show()\n",
    "\n",
    "result = adfuller(data['EXINUS'])\n",
    "print(\"Adfuller statistics:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "#print(f'ADF Statistic: {result[0]}, p-value: {result[1]}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(data['EXINUS'], lags=30, ax=ax1)\n",
    "plot_pacf(data['EXINUS'], lags=30, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvPlC5tu50YR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9hzWPMi57WF"
   },
   "outputs": [],
   "source": [
    "#r programming\n",
    "#2\n",
    "install.packages(\"readr\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"Hmisc\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"datarium\")\n",
    "install.packages(\"caret\")\n",
    "library(readr)#reading a csv file\n",
    "library(dplyr)#data wrangling\n",
    "library(Hmisc)#data description\n",
    "library(ggplot2)#data visualization\n",
    "library(datarium)#dataset\n",
    "library(caret)#macine learning library for splitting on training and test\n",
    "data(\"marketing\", package=\"datarium\")\n",
    "marketing_plan<-marketing\n",
    "marketing_plan\n",
    "\n",
    "#1\n",
    "marketing_plan %>% ggplot(aes(x = youtube, y = sales)) +  geom_point() +\n",
    "  labs(x = \"Spending on YouTube ads\",y = \"Sales\", title = \"Graph 1: Relationship between YouTube ads and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "marketing_plan %>% ggplot(aes(x = facebook, y = sales)) +  geom_point() +\n",
    "  labs(x = \"facebook\",y = \"Sales\", title = \"Graph 2: Relationship between facebook and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "marketing_plan %>% ggplot(aes(x = newspaper, y = sales)) +  geom_point() +\n",
    "  labs(x = \"newspaper\",y = \"Sales\", title = \"Graph 3: Relationship between newspaper and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "\n",
    "#4\n",
    "set.seed(1)\n",
    "train_indices<-createDataPartition(y=marketing[[\"sales\"]],\n",
    "                                   p=0.8,\n",
    "                                   list = FALSE)\n",
    "train_listings<-marketing[train_indices,]\n",
    "test_listings<-marketing[-train_indices,]\n",
    "\n",
    "#3\n",
    "#observation\n",
    "#we can see that the p value for youtube and facebook is extremely small,\n",
    "#which means that we reject the null hypothesis that the youtube and facbook donot impact sales\n",
    "#on the other hand, p value for newspaper is greater than 0.05,\n",
    "#which means it is not a significant value. we fail to reject the null hypothesis\n",
    "#that there is any significant relationship between newspaper ads and sales.\n",
    "#I will create second model to exclude the variable newspaper.\n",
    "\n",
    "#5\n",
    "#case1:considering youtube,facebook,newspaper versus sales\n",
    "model_0<-lm(sales~youtube+facebook+newspaper,data=train_listings)\n",
    "summary(model_0)\n",
    "model_1<-lm(sales~youtube+facebook,data=train_listings)\n",
    "summary(model_1)\n",
    "model_2<-lm(sales~facebook+I(facebook^2)+youtube+I(youtube^2),data=train_listings)\n",
    "summary(model_2)\n",
    "model_3<-lm(sales~facebook+poly(youtube,5),data=train_listings)\n",
    "summary(model_3)\n",
    "model_4<-lm(sales~facebook+poly(youtube,3)+facebook*youtube,data=train_listings)\n",
    "summary(model_4)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
