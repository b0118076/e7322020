{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6-ifF-lNDtc"
   },
   "outputs": [],
   "source": [
    "#CA-1 1st\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from the local CSV file\n",
    "file_path = \"C:/Users/RITHVI/Desktop/pred_final/IPG2211A2N.csv\"\n",
    "data = pd.read_csv(file_path, parse_dates=['DATE'], index_col='DATE')\n",
    "\n",
    "# a. Resample the series and choose an essential graph to visualize the month-wise five number summary.\n",
    "monthly_summary = data.resample('M').apply(lambda x: x.describe().loc['50%'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(monthly_summary['IPG2211A2N'])\n",
    "plt.title('Month-wise Five Number Summary for Industrial Production Index')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Index Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# b. Enumerate the components of a time series and isolate the components using additive and multiplicative models.\n",
    "# Decompose the time series using additive and multiplicative models\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Use additive decomposition\n",
    "result_add = seasonal_decompose(data['IPG2211A2N'], model='additive', extrapolate_trend='freq')\n",
    "\n",
    "# Use multiplicative decomposition\n",
    "result_mult = seasonal_decompose(data['IPG2211A2N'], model='multiplicative', extrapolate_trend='freq')\n",
    "\n",
    "# Isolate the components\n",
    "trend_add = result_add.trend\n",
    "seasonal_add = result_add.seasonal\n",
    "residual_add = result_add.resid\n",
    "\n",
    "trend_mult = result_mult.trend\n",
    "seasonal_mult = result_mult.seasonal\n",
    "residual_mult = result_mult.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYAkeT0NNDwf"
   },
   "outputs": [],
   "source": [
    "#CA1 - 2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data from the local CSV file\n",
    "file_path = \"/content/IPG2211A2N.csv\"\n",
    "data = pd.read_csv(file_path, parse_dates=['DATE'], index_col='DATE')\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Perform ADF test on the original series\n",
    "result = adfuller(data['IPG2211A2N'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "# Plot the original time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data['IPG2211A2N'])\n",
    "plt.title('Original Time Series - Industrial Production Index')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Index Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make the time series stationary using differencing\n",
    "data_diff = data['IPG2211A2N'].diff().dropna()\n",
    "\n",
    "# Plot the differenced time series\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_diff)\n",
    "plt.title('Differenced Time Series - Industrial Production Index')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Differenced Index Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYjUoxZaNozA"
   },
   "outputs": [],
   "source": [
    "#Pyspark\n",
    "\n",
    "pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Step 1: Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"LogisticRegressionExample\").getOrCreate()\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "data = spark.read.csv(\"C:/Users/RITHVI/Desktop/pred_final/Log_Reg_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Perform descriptive statistics\n",
    "data.describe().show()\n",
    "\n",
    "# Step 4: Assign probability values (0 and 1) for class 0 and class 1 in 'Status' column\n",
    "data = data.withColumn(\"Status\", data[\"Status\"].cast(\"double\"))\n",
    "\n",
    "# Step 5: Convert categorical columns to numeric using StringIndexer\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\")\n",
    "    for col in [\"Country\", \"Platform\"]\n",
    "]\n",
    "indexers.append(StringIndexer(inputCol=\"Repeat_Visitor\", outputCol=\"Repeat_Visitor_index\", handleInvalid=\"keep\"))\n",
    "\n",
    "# Fit and transform the data with the StringIndexer\n",
    "for indexer in indexers:\n",
    "    data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Step 6: Prepare data for logistic regression\n",
    "# Select features and label columns\n",
    "feature_cols = [\"Country_index\", \"Age\", \"Repeat_Visitor_index\", \"Platform_index\", \"Web_pages_viewed\"]\n",
    "label_col = \"Status\"\n",
    "\n",
    "# Create a vector assembler to combine features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Step 7: Split the data into training and test datasets\n",
    "(training_data, test_data) = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 8: Build and fit the logistic regression model\n",
    "lr = LogisticRegression(labelCol=label_col)\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# Step 9: Make predictions on the test dataset\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = predictions.groupBy(\"Status\", \"prediction\").count()\n",
    "print(\"Confusion Matrix:\")\n",
    "conf_matrix.show()\n",
    "\n",
    "# Precision, recall, and F1 score\n",
    "TP = predictions[(predictions.Status == 1) & (predictions.prediction == 1)].count()\n",
    "TN = predictions[(predictions.Status == 0) & (predictions.prediction == 0)].count()\n",
    "FP = predictions[(predictions.Status == 0) & (predictions.prediction == 1)].count()\n",
    "FN = predictions[(predictions.Status == 1) & (predictions.prediction == 0)].count()\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clqiJINgj6Xc"
   },
   "outputs": [],
   "source": [
    "#Superstore\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel('/content/Superstore.xls')\n",
    "\n",
    "office_supplies = df[df['Category'] == 'Office Supplies']\n",
    "office_supplies['Order Date'] = pd.to_datetime(office_supplies['Order Date'])\n",
    "# Monthly sales\n",
    "monthly_sales = office_supplies.groupby(pd.Grouper(key='Order Date', freq='M')).sum()['Sales']\n",
    "\n",
    "# a. Stationarity check\n",
    "plt.plot(monthly_sales, label='Original')\n",
    "plt.plot(monthly_sales.rolling(window=12).mean(), label='Rolling Mean')\n",
    "plt.plot(monthly_sales.rolling(window=12).std(), label='Rolling Std')\n",
    "plt.legend()\n",
    "plt.title('Rolling Mean & Standard Deviation')\n",
    "plt.show()\n",
    "\n",
    "# Augmented Dickey-Fuller test\n",
    "result = adfuller(monthly_sales)\n",
    "print('ADF Statistic (a):', result[0])\n",
    "print('p-value (a):', result[1])\n",
    "print('Critical Values (a):', result[4])\n",
    "\n",
    "# b. Determine order of differencing, d\n",
    "d = 1\n",
    "\n",
    "plt.plot(monthly_sales)\n",
    "plt.title('Monthly Sales')\n",
    "plt.xlabel('Order Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# c. Determine order p for AR(p)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_pacf(monthly_sales, lags=min(24, len(monthly_sales)-1), alpha=0.05)\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()\n",
    "\n",
    "# Assuming p = 2\n",
    "p = 2\n",
    "\n",
    "# d. Determine order q for MA(q)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_acf(monthly_sales, lags=30, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "plt.show()\n",
    "\n",
    "# Assuming q = 2\n",
    "q = 2\n",
    "\n",
    "# e. Fit ARIMA model and forecast\n",
    "model = ARIMA(monthly_sales, order=(p, d, q))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=len(monthly_sales))\n",
    "\n",
    "# Plot original data and forecast\n",
    "plt.plot(monthly_sales, label='Original')\n",
    "plt.plot(monthly_sales.index, forecast, label='Forecast', linestyle='--')\n",
    "plt.legend()\n",
    "plt.title('ARIMA Forecast')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model accuracy\n",
    "mse = mean_squared_error(monthly_sales, forecast)\n",
    "print('Mean Squared Error (MSE) (e):', mse)\n",
    "\n",
    "#######\n",
    "#e.\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel(\"C:\\\\Users\\\\91636\\\\Downloads\\\\Superstore (1).xls\")\n",
    "office_supplies_sales = data[data['Category'] == 'Office Supplies'][['Order Date', 'Sales']]\n",
    "office_supplies_sales = office_supplies_sales.set_index('Order Date').sort_index()\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(office_supplies_sales['Sales'], order=(2, 1, 2))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Summary\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Forecasting\n",
    "forecast = model_fit.forecast(steps=10)\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsKQYjOT9KRd"
   },
   "outputs": [],
   "source": [
    "#ANN\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/RITHVI/Desktop/pred_final/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "encoder = LabelEncoder()\n",
    "scaler = StandardScaler()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "df[categorical_cols] = df[categorical_cols].apply(encoder.fit_transform)\n",
    "numerical_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=['customerID', 'Churn'])\n",
    "y = df['Churn']\n",
    "\n",
    "# Define the ANN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=34, epochs=100)\n",
    "\n",
    "new_customer_info = np.array([[10, 50.0, 500.0]])  # Example: Tenure=10, MonthlyCharges=50.0, TotalCharges=500.0\n",
    "\n",
    "# Scale the new customer information\n",
    "new_customer_info_scaled = scaler.transform(new_customer_info)\n",
    "\n",
    "# Expand the new_customer_info_scaled array to match the input shape expected by the model\n",
    "new_customer_info_scaled_expanded = np.hstack((new_customer_info_scaled, np.zeros((new_customer_info_scaled.shape[0], 16))))\n",
    "\n",
    "# Predict churn for the new customer\n",
    "prediction = model.predict(new_customer_info_scaled_expanded)\n",
    "\n",
    "if prediction > 0.5:\n",
    "    print(\"The new customer is predicted to churn.\")\n",
    "else:\n",
    "    print(\"The new customer is predicted not to churn.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37uhqToJkLgU"
   },
   "outputs": [],
   "source": [
    "#Spam\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/RITHVI/Desktop/pred_final/Spam.csv\", encoding='latin-1')\n",
    "\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/RITHVI/Desktop/pred_final/Spam.csv\", encoding='latin-1')\n",
    "\n",
    "df = df[['v1', 'v2']]\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "x = vectorizer.fit_transform(df['message'])\n",
    "\n",
    "\n",
    "y = df['label']\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x, y)\n",
    "\n",
    "def evaluate_classifier(clf, x_test, y_test):\n",
    "    y_pred = clf.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, pos_label='spam')\n",
    "    recall = recall_score(y_test, y_pred, pos_label='spam')\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_classifier(clf, x, y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"precision: {precision:.2f}\")\n",
    "print(f\"recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHbKvNqn5mBi"
   },
   "outputs": [],
   "source": [
    "#India exchange rate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "data = pd.read_excel(\"C:/Users/RITHVI/Downloads/India_Exchange_Rate_Dataset.xls\")\n",
    "\n",
    "data['EXINUS'].plot(figsize=(12, 6), title='India Exchange Rate')\n",
    "plt.show()\n",
    "\n",
    "data['EXINUS'].rolling(window=12).mean().plot(figsize=(12, 6), title='India Exchange Rate with 12-month SMA')\n",
    "plt.show()\n",
    "\n",
    "data['EXINUS'].ewm(span=12).mean().plot(figsize=(12, 6), title='India Exchange Rate with 12-month EWMA')\n",
    "plt.show()\n",
    "\n",
    "result = adfuller(data['EXINUS'])\n",
    "print(\"Adfuller statistics:\", result[0])\n",
    "print(\"p-value:\", result[1])\n",
    "#print(f'ADF Statistic: {result[0]}, p-value: {result[1]}')\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(data['EXINUS'], lags=30, ax=ax1)\n",
    "plot_pacf(data['EXINUS'], lags=30, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvPlC5tu50YR"
   },
   "outputs": [],
   "source": [
    "#Fitbit\n",
    "\n",
    "pip install pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor as SparkRandomForestRegressor, LinearRegression as SparkLinearRegression\n",
    "\n",
    "# Step 1: Create SQLite database \"fitbit.db\" and load the fitbitdf.csv file\n",
    "conn = sqlite3.connect('fitbit.db')\n",
    "\n",
    "# Load the fitbitdf.csv file into the SQLite database\n",
    "df = pd.read_csv('/content/fitbit_df.csv')\n",
    "df.to_sql('fitbit_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Step 2: Perform exploratory analysis - calculate correlation measures\n",
    "correlation_matrix = df.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Step 3: Predict calories using tree-based machine learning models\n",
    "X = df.drop(columns=['Calories'])\n",
    "y = df['Calories']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "rf_model = RandomForestRegressor().fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Regression Results:\")\n",
    "print(f\"MSE: {mse_rf}, R2: {r2_rf}\")\n",
    "\n",
    "# Step 4: Choose best performing model for PySpark implementation\n",
    "spark = SparkSession.builder.appName(\"FitBitModel\").getOrCreate()\n",
    "df_spark = spark.createDataFrame(df)\n",
    "assembler = VectorAssembler(inputCols=[str(col) for col in df.columns[:-1]], outputCol='features')\n",
    "vector_df = assembler.transform(df_spark)\n",
    "train_df, test_df = vector_df.randomSplit([0.8, 0.2], seed=42)\n",
    "rf_model_spark = SparkRandomForestRegressor(featuresCol='features', labelCol='Calories').fit(train_df)\n",
    "predictions_spark_rf = rf_model_spark.transform(test_df).select('Calories', 'prediction').toPandas()\n",
    "mse_spark_rf = mean_squared_error(predictions_spark_rf['Calories'], predictions_spark_rf['prediction'])\n",
    "r2_spark_rf = r2_score(predictions_spark_rf['Calories'], predictions_spark_rf['prediction'])\n",
    "print(\"Random Forest Regression Results using PySpark:\")\n",
    "print(f\"MSE: {mse_spark_rf}, R2: {r2_spark_rf}\")\n",
    "\n",
    "# Step 5: Predict calories using linear machine learning models\n",
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "print(\"Linear Regression Results:\")\n",
    "print(f\"MSE: {mse_lr}, R2: {r2_lr}\")\n",
    "\n",
    "# Step 6: Close Spark session and SQLite connection\n",
    "spark.stop()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9hzWPMi57WF"
   },
   "outputs": [],
   "source": [
    "#r programming\n",
    "#2\n",
    "install.packages(\"readr\")\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"Hmisc\")\n",
    "install.packages(\"ggplot2\")\n",
    "install.packages(\"datarium\")\n",
    "install.packages(\"caret\")\n",
    "library(readr)#reading a csv file\n",
    "library(dplyr)#data wrangling\n",
    "library(Hmisc)#data description\n",
    "library(ggplot2)#data visualization\n",
    "library(datarium)#dataset\n",
    "library(caret)#macine learning library for splitting on training and test\n",
    "data(\"marketing\", package=\"datarium\")\n",
    "marketing_plan<-marketing\n",
    "marketing_plan\n",
    "\n",
    "#1\n",
    "marketing_plan %>% ggplot(aes(x = youtube, y = sales)) +  geom_point() +\n",
    "  labs(x = \"Spending on YouTube ads\",y = \"Sales\", title = \"Graph 1: Relationship between YouTube ads and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "marketing_plan %>% ggplot(aes(x = facebook, y = sales)) +  geom_point() +\n",
    "  labs(x = \"facebook\",y = \"Sales\", title = \"Graph 2: Relationship between facebook and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "marketing_plan %>% ggplot(aes(x = newspaper, y = sales)) +  geom_point() +\n",
    "  labs(x = \"newspaper\",y = \"Sales\", title = \"Graph 3: Relationship between newspaper and sales\") +  stat_smooth(se = FALSE) +   theme(panel.background = element_rect(fill = \"white\", colour = \"grey50\"))\n",
    "\n",
    "#4\n",
    "set.seed(1)\n",
    "train_indices<-createDataPartition(y=marketing[[\"sales\"]],\n",
    "                                   p=0.8,\n",
    "                                   list = FALSE)\n",
    "train_listings<-marketing[train_indices,]\n",
    "test_listings<-marketing[-train_indices,]\n",
    "\n",
    "#3\n",
    "#observation\n",
    "#we can see that the p value for youtube and facebook is extremely small,\n",
    "#which means that we reject the null hypothesis that the youtube and facbook donot impact sales\n",
    "#on the other hand, p value for newspaper is greater than 0.05,\n",
    "#which means it is not a significant value. we fail to reject the null hypothesis\n",
    "#that there is any significant relationship between newspaper ads and sales.\n",
    "#I will create second model to exclude the variable newspaper.\n",
    "\n",
    "#5\n",
    "#case1:considering youtube,facebook,newspaper versus sales\n",
    "model_0<-lm(sales~youtube+facebook+newspaper,data=train_listings)\n",
    "summary(model_0)\n",
    "model_1<-lm(sales~youtube+facebook,data=train_listings)\n",
    "summary(model_1)\n",
    "model_2<-lm(sales~facebook+I(facebook^2)+youtube+I(youtube^2),data=train_listings)\n",
    "summary(model_2)\n",
    "model_3<-lm(sales~facebook+poly(youtube,5),data=train_listings)\n",
    "summary(model_3)\n",
    "model_4<-lm(sales~facebook+poly(youtube,3)+facebook*youtube,data=train_listings)\n",
    "summary(model_4)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
